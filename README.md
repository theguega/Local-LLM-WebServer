# Local-LLM-Chat

This project implements a ChatGPT-like interface build with Rust for backend and Next.js for frontend.
You can use it with you own LLMs and call it using CLI into main.rs.

<div id="image-table">
    <img src="frontend/public/Local LLM WebServer.jpeg" width="450" height="auto" align="center">
</div>

## Launching the project

First you need to build the project with cargo and npm and then run the project using the following commands :

```bash
(cd backend && cargo run) | (cd frontend && npm run dev)
```

---

Credits : Theo GUEGAN
